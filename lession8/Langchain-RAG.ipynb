{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a331fd6",
   "metadata": {},
   "source": [
    "## RAG example with Langchain, Redis, and HFTGI\n",
    "\n",
    "Requirements:\n",
    "- A Redis cluster and Database where documents have been injected\n",
    "- All information for connecting to the redis cluster and database, index name and schema file.\n",
    "- An inference endpoint served with Hugging Face Text Generation Inference server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4537b",
   "metadata": {},
   "source": [
    "#### Bases parameters, Inference server and Redis info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51baf1a6-4111-4b40-b43a-833438bdc222",
   "metadata": {
    "tags": []
   },
   "source": [
    "inference_server_url = \"http://hf-tgi.llm-hosting.svc.cluster.local:3000/\"\n",
    "redis_url = \"redis://server:port\"\n",
    "index_name = \"docs\"\n",
    "schema_name = \"redis_schema.yaml\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "82970898",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e11d23-c0ad-4875-b67f-149fc8b14725",
   "metadata": {
    "tags": []
   },
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.redis import Redis\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fe4c1b1a",
   "metadata": {},
   "source": [
    "#### Initialize the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6a3e3-5ccd-441e-b80d-427555d9e9f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "rds = Redis.from_existing_index(\n",
    "    embeddings,\n",
    "    redis_url=redis_url,\n",
    "    index_name=index_name,\n",
    "    schema=schema_name\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b72a3a2b",
   "metadata": {},
   "source": [
    "#### Initialize query chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fd396-0798-45c5-8969-6b6ede134c77",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# NOTE: This template syntax is specific to Llama2\n",
    "template=\"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant.\n",
    "You will be given a question you need to answer, and a context to provide you with information. You must answer the question based as much as possible on this context.\n",
    "Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Question: {question}\n",
    "Context: {context} [/INST]\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url,\n",
    "    max_new_tokens=512,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.175,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=rds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4, \"distance_threshold\": 0.5}),\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "                                       return_source_documents=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3a45ad23",
   "metadata": {},
   "source": [
    "#### Query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d2fd1-f36c-409d-8e52-ec6d23a56ad1",
   "metadata": {
    "tags": []
   },
   "source": [
    "question = \"How can I work with GPU and taints?\"\n",
    "result = qa_chain({\"query\": question})"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "97d75d0c",
   "metadata": {},
   "source": [
    "#### Retrieve source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda357e-558a-4879-8ad8-21f0567f2f2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "def remove_duplicates(input_list):\n",
    "    unique_list = []\n",
    "    for item in input_list:\n",
    "        if item.metadata['source'] not in unique_list:\n",
    "            unique_list.append(item.metadata['source'])\n",
    "    return unique_list\n",
    "\n",
    "results = remove_duplicates(result['source_documents'])\n",
    "\n",
    "for s in results:\n",
    "    print(s)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
